{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline\n",
    "# plt.rcParams[\"animation.html\"]='jshtml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  He Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_normal(X,y,hidden):\n",
    "    W1=np.random.normal(loc=0,scale=np.sqrt(2/(X.shape[1])),size=(X.shape[1],hidden[0]))\n",
    "    b1=0\n",
    "    W2=np.random.normal(loc=0,scale=np.sqrt(2/(hidden[0])),size=(hidden[0],len(np.unique(y))))\n",
    "    b2=0\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_uniform(X,y,hidden):\n",
    "    W1=np.random.uniform(low=-np.sqrt(6/(X.shape[1])),high=np.sqrt(6/(X.shape[1])),size=(X.shape[1],hidden[0]))\n",
    "    b1=0\n",
    "    W2=np.random.uniform(low=-np.sqrt(6/hidden[0]),high=np.sqrt(6/hidden[0]),size=(hidden[0],len(np.unique(y))))\n",
    "    b2=0\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorat Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorat_normal(X,y,hidden):\n",
    "    W1=np.random.normal(loc=0,scale=np.sqrt(2/(X.shape[1]+hidden[0])),size=(X.shape[1],hidden[0]))\n",
    "    b1=0\n",
    "    W2=np.random.normal(loc=0,scale=np.sqrt(2/(hidden[0]+len(np.unique(y)))),size=(hidden[0],len(np.unique(y))))\n",
    "    b2=0\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorat Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorat_uniform(X,y,hidden):\n",
    "    W1=np.random.uniform(low=-np.sqrt(6/(X.shape[1]+hidden[0])),high=np.sqrt(6/(X.shape[1])),size=(X.shape[1],hidden[0]))\n",
    "    b1=0\n",
    "    W2=np.random.uniform(low=-np.sqrt(6/(hidden[0]+len(np.unique(y)))),high=np.sqrt(6/(hidden[0]+len(np.unique(y)))),size=(hidden[0],len(np.unique(y))))\n",
    "    b2=0\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_igmoid (Z):\n",
    "    return Z *(1-Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_ReLU(Z):\n",
    "    return  np.where(Z>0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expA=np.exp(Z-np.max(Z))\n",
    "    return expA/expA.sum(axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hoty(y):\n",
    "    z=np.zeros((len(y),len(np.unique(y))),dtype=int)\n",
    "    for i in range(len(y)):\n",
    "        z[i,y[i]]=1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Probagation\n",
    "\n",
    "> Note: If we want we can change the activation function manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_probagation(X,W1,b1,W2,b2):\n",
    "    #Input Layer\n",
    "    Z1=np.dot(X,W1)+b1\n",
    "    \n",
    "    # Hidden Layer\n",
    "    Activation1=ReLU(Z1)\n",
    "    \n",
    "    Z2=np.dot(Activation1,W2)+b2\n",
    "    \n",
    "    # Output layer\n",
    "    YP=softmax(Z2)\n",
    "    \n",
    "    return Z1,Activation1,Z2,YP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Probagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Back_probagation(X,y,W2,A1,YP,derivative_act):\n",
    "    dW2=(-(y-YP)).T.dot(A1)\n",
    "    db2=(-(y-YP)).sum(axis=0)\n",
    "    dW1=(-((y-YP).dot(W2.T)*(derivative_act))).T.dot(X)\n",
    "    db1=(-((y-YP).dot(W2.T)*derivative_act)).sum(axis=0)\n",
    "    return dW2,db2,dW1,db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(W1,b1,W2,b2,dW2,db2,dW1,db1,lr):\n",
    "    W1=W1-(lr*dW1.T)\n",
    "    b1=b1-(lr*db1)\n",
    "    W2=W2-(lr*dW2.T)\n",
    "    b2=b2-(lr*db2)\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(X,W1,b1,W2,b2):\n",
    "    _,_,_,YP=forward_probagation(X,W1,b1,W2,b2)\n",
    "    res=np.zeros(YP.shape[0],dtype=int)\n",
    "    for i in range(YP.shape[0]):\n",
    "        if YP[i,0]>YP[i,1]:\n",
    "            res[i]=0\n",
    "        else:\n",
    "            res[i]=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Here we need to change the derivative of the activation function manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can change the initialization method here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainining(X,y,n_hidden,lr,iterration,plot=False):\n",
    "\n",
    "# We can see the step by step transformation by setting plot = True    \n",
    "    \n",
    "    if plot:\n",
    "        fig=plt.figure(1,(13,5))\n",
    "        ax1=fig.add_subplot(141)\n",
    "        ax2=fig.add_subplot(142)\n",
    "        ax3=fig.add_subplot(143)\n",
    "        ax4=fig.add_subplot(144)\n",
    "    \n",
    "# Weight initialization\n",
    "\n",
    "#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "    W1,b1,W2,b2=he_normal(X,y,n_hidden)       # Try different techniques\n",
    "\n",
    "#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    \n",
    "# Converting y into one hot vector\n",
    "    y1=one_hoty(y)\n",
    "    \n",
    "    for i in range(iterration):\n",
    "        \n",
    "# Forward pass\n",
    "        Z1,A1,Z2,YP=forward_probagation(X,W1,b1,W2,b2)\n",
    "        \n",
    "# Computing Cost\n",
    "        cost=-(np.sum(y1*(np.log(YP)))).sum()\n",
    "\n",
    "# Ploting the transformation\n",
    "        if plot:\n",
    "            ax1.scatter(X[:,0],X[:,1],c=y)\n",
    "            ax2.scatter(A1[:,0],A1[:,1],c=y)\n",
    "            ax3.scatter(Z2[:,0],Z2[:,1],c=y)\n",
    "            ax4.scatter(YP[:,0],YP[:,1],c=y)\n",
    "            fig.canvas.draw()\n",
    "\n",
    "# Computing gradient of activation function             >>>>>> (\" Need to change when we change the activation function\")\n",
    "        derivative_activation=derivative_ReLU(A1)\n",
    "    \n",
    "# Backward function\n",
    "        dW2,db2,dW1,db1=Back_probagation(X,y1,W2,A1,YP,derivative_activation)\n",
    "    \n",
    "# Updating Weight\n",
    "        W1,b1,W2,b2=update_weights(W1,b1,W2,b2,dW2,db2,dW1,db1,lr)\n",
    "        \n",
    "#     print(cost)\n",
    "    return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Xor function\n",
    "\n",
    "X=np.array([[1,1],[1,0],[0,1],[0,0],[1,1],[1,0],[0,1],[0,0],[1,1],[1,0],[0,1],[0,0]])\n",
    "y=np.array([0,1,1,0]*3)\n",
    "\n",
    "# Creating DF\n",
    "data=pd.DataFrame(data=X,columns=[f'X{i}' for i in range(X.shape[1])])\n",
    "data[\"y\"]=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1,b1,W2,b2=trainining(X,y,[10],0.001,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=predict1(X,W1,b1,W2,b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_predicted==y)/len(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
